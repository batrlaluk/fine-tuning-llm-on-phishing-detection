{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2697e5-a68a-48dd-ab46-07f4e19e0514",
   "metadata": {},
   "source": [
    "# Fine-tuning of LLMs\n",
    "(notebook for CPU)\n",
    "\n",
    "--------------------\n",
    "\n",
    "Prepared by Lukáš Bátrla, AI Researcher at Cisco Systems.\n",
    "\n",
    "If you find this topic interesting and want to collaborate e.g. on a thesis, feel free to send me a message to lbatrla@cisco.com or to [my LinkedIn profile](https://www.linkedin.com/in/lukas-batrla/).\n",
    "\n",
    "--------------------\n",
    "\n",
    "## The structure of today's lecture\n",
    "\n",
    "1. Problem introduction\n",
    "2. Data preparation and splitting\n",
    "3. In-context learning\n",
    "4. Full fine-tuning\n",
    "5. Parameter efficient fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c653c95d",
   "metadata": {},
   "source": [
    "# Phishing Email Classification using LLM\n",
    "This notebook demonstrates the usage of an LLM model on the phishing email classification. We'll start with introduction of the dataset and then apply LLM on it.\n",
    "In this notebook, we'll introduce several fine-tuning techniques that will help us incrementally improve the LLM's competency to classify emails as phishing or benign.\n",
    "\n",
    "<img src=\"images/email-phishing-example.jpg\" width=\"700\">\n",
    "\n",
    "**Disclaimer:** The notes about the general fine-tuning methods in this notebook are inspired by concepts I learned in the [Generative AI with Large Language Models](https://www.deeplearning.ai/courses/generative-ai-with-llms/) course by DeepLearning.AI. However, the notes, the dataset, implementation, and model choices have been adapted for phishing detection.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. Load and prepare the phishing email dataset with stratified splitting into train/validation/test\n",
    "2. Difference between various **in-context-learning** methods - zero-shot (a.k.a. baseline model), one-shot, and few-shot method\n",
    "3. Configure and run **full fine-tuning**\n",
    "4. Configure and run **LoRA fine-tuning**\n",
    "\n",
    "## Training Configuration (CPU-Optimized):\n",
    "- **Model**: FLAN-T5 small - 77M parameters (full parameter training)\n",
    "- **Device Options**: NVIDIA GPU (CUDA), Apple GPU (MPS), or CPU\n",
    "- **Training Time**: ~1 hour on GPU, ~4-8 hours on CPU\n",
    "- **Memory Requirements**: ~4-8GB RAM\n",
    "- **CPU Optimizations**: Reduced dataset (20%), smaller batches, optimized evaluation subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a981494-9f8c-4675-ac9c-91aba8f8b9cb",
   "metadata": {},
   "source": [
    "## 0. Device Configuration\n",
    "\n",
    "Training on GPU is significantly faster than CPU. The following code automatically detects and uses the best available device.\n",
    "\n",
    "**Priority**: CUDA (NVIDIA) > MPS (Apple Silicon) > CPU\n",
    "\n",
    "- **CUDA**: NVIDIA GPU acceleration - fastest option (~1-2 hours)\n",
    "- **MPS**: Metal Performance Shaders for M1/M2/M3/M4 Macs (~2-4 hours)\n",
    "- **CPU**: Fallback option (~4-8 hours with optimized settings for 20% dataset)\n",
    "\n",
    "**Note**: This notebook uses CPU-optimized parameters including reduced dataset (20%), smaller batch sizes, and limited evaluation samples to make training practical on CPU.\n",
    "\n",
    "Set `USE_MPS = False` to force CPU-only training if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303109f6-26ca-414e-bc8e-4f265a15dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "USE_MPS = False  # Set to False to force CPU training\n",
    "\n",
    "# Device detection\n",
    "if torch.cuda.is_available():\n",
    "    device_type = \"cuda\"\n",
    "    print(f\"Using device: CUDA (NVIDIA GPU) - {torch.cuda.get_device_name(0)}\")\n",
    "elif USE_MPS and torch.backends.mps.is_available():\n",
    "    device_type = \"mps\"\n",
    "    print(\"Using device: MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device_type = \"cpu\"\n",
    "    print(\"Using device: CPU (⚠️ Full fine-tuning will be slow ~100+ hours)\")\n",
    "\n",
    "device = torch.device(device_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96a9fd",
   "metadata": {},
   "source": [
    "## 1. Load Phishing Email Dataset\n",
    "\n",
    "We will use the [phishing-email-dataset](https://huggingface.co/datasets/zefang-liu/phishing-email-dataset) from HuggingFace, which contains over 18,000 emails labeled as either \"Safe Email\" or \"Phishing Email\".\n",
    "\n",
    "**Expected**: ~18,650 examples with roughly 60% safe emails and 40% phishing emails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f55bff5",
   "metadata": {},
   "source": [
    "### 1.1. Normalize columns and labels\n",
    "The dataset will be:\n",
    "- Normalized with consistent column names (`text` and `label`)\n",
    "- Augmented with a numeric `label_id` for stratified splitting (0=Safe, 1=Phishing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83719d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"zefang-liu/phishing-email-dataset\")\n",
    "ds = dataset[\"train\"]\n",
    "\n",
    "# Normalize column names\n",
    "ds = ds.rename_columns({\"Email Text\": \"text\", \"Email Type\": \"label\"})\n",
    "\n",
    "# Add numeric label for stratified splitting\n",
    "label_names = [\"Safe Email\", \"Phishing Email\"]\n",
    "label_to_id = {name: i for i, name in enumerate(label_names)}\n",
    "ds = ds.map(lambda x: {\"label_id\": label_to_id[x[\"label\"]]})\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859a6f06",
   "metadata": {},
   "source": [
    "### 1.2. Create Stratified Train/Validation/Test Splits\n",
    "\n",
    "To ensure fair evaluation, we perform **stratified splitting** which maintains the same class distribution (Safe vs Phishing ratio) across train, validation, and test sets.\n",
    "\n",
    "**Split ratios:**\n",
    "- Training: 80% (~14,900 examples)\n",
    "- Validation: 10% (~1,860 examples)  \n",
    "- Test: 10% (~1,860 examples)\n",
    "\n",
    "This prevents training on an imbalanced split that could bias the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(dataset, label_column, train_size=0.8, val_size=0.1, seed=42):\n",
    "    \"\"\"Stratified split maintaining class distribution across all splits.\"\"\"\n",
    "    labels = np.array(dataset[label_column])\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx_train, idx_val, idx_test = [], [], []\n",
    "\n",
    "    for label_value in np.unique(labels):\n",
    "        idx = np.where(labels == label_value)[0]\n",
    "        rng.shuffle(idx)\n",
    "        n = len(idx)\n",
    "        n_train = int(train_size * n)\n",
    "        n_val = int(val_size * n)\n",
    "        idx_train.extend(idx[:n_train])\n",
    "        idx_val.extend(idx[n_train:n_train+n_val])\n",
    "        idx_test.extend(idx[n_train+n_val:])\n",
    "\n",
    "    rng.shuffle(idx_train)\n",
    "    rng.shuffle(idx_val)\n",
    "    rng.shuffle(idx_test)\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": dataset.select(idx_train),\n",
    "        \"validation\": dataset.select(idx_val),\n",
    "        \"test\": dataset.select(idx_test)\n",
    "    })\n",
    "\n",
    "ds_splits = stratified_split(ds, \"label_id\", train_size=0.8, val_size=0.1)\n",
    "ds_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abbb2af",
   "metadata": {},
   "source": [
    "## 2. In-Context Learning: Zero-Shot, One-Shot, and Few-Shot methods\n",
    "- In-context learning refers to a model's ability to solve a task by reasoning over examples given directly within the prompt, rather than through weight updates or fine-tuning.\n",
    "- Before fine-tuning, we assess the model's **in-context learning** performance — seeing how well it can classify emails when shown examples in the prompt alone.\n",
    "\n",
    "<img src=\"images/in-context-learning-diagram.jpg\">\n",
    "\n",
    "Why use in-context learning?\n",
    "- Improves model performance on new tasks without requiring additional model training or parameter updates (saving time and compute)\n",
    "- Enables flexible, task-specific adaptation even when labeled data is limited or unavailable for fine-tuning\n",
    "- Supports rapid prototyping and testing of prompts before committing to more resource-intensive fine-tuning\n",
    "- Useful in scenarios where model weights cannot be modified (e.g., API usage, deployment constraints)\n",
    "\n",
    "We test three approaches:\n",
    "- **Zero-Shot**: No examples (pure baseline)\n",
    "- **One-Shot**: 1 example in the prompt  \n",
    "- **Few-Shot**: 4 examples in the prompt (2 safe, 2 phishing)\n",
    "\n",
    "This comparison helps us understand:\n",
    "1. The model's baseline capability on our task\n",
    "2. How much improvement we can get \"for free\" with just prompting\n",
    "3. The value of fine-tuning vs. in-context learning\n",
    "\n",
    "**Note**: Depending on the task, few-shot (and even one-shot) approach can easily fill the whole context window of the model.\n",
    "\n",
    "**Note**: FLAN-T5 is instruction-tuned, so it performs reasonably well even in zero-shot mode. This makes it much more suitable for fine-tuning on classification tasks compared to base models like GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Choose model based on available memory\n",
    "# model_id = \"google/flan-t5-base\"  # 250M params\n",
    "model_id = \"google/flan-t5-small\"  # 77M params (current) - Seq2Seq, instruction-tuned\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"cpu\")\n",
    "\n",
    "# Create output directory name based on model\n",
    "model_name_clean = model_id.split('/')[-1].lower().replace('_', '-')\n",
    "output_full_finetuning_dir = f\"./{model_name_clean}_finetuned_phishing\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ZERO-SHOT: No Examples\n",
    "# ============================================================\n",
    "\n",
    "def classify_zero_shot(text):\n",
    "    \"\"\"Zero-shot classification with simple prompt.\"\"\"\n",
    "    # Truncate email to fit context better\n",
    "    text_snippet = text[:400]\n",
    "    \n",
    "    prompt = f\"Classify this email as 'Safe Email' or 'Phishing Email': {text_snippet}\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=10,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    ans = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    prediction = ans.strip()\n",
    "    return prediction\n",
    "\n",
    "# ============================================================\n",
    "# ONE-SHOT: 1 Example\n",
    "# ============================================================\n",
    "\n",
    "def classify_one_shot(text, example_text, example_label):\n",
    "    \"\"\"\n",
    "    One-shot classification: provide ONE example in the prompt.\n",
    "    \"\"\"\n",
    "    # Truncate texts\n",
    "    ex_snippet = example_text[:150]\n",
    "    test_snippet = text[:250]\n",
    "    \n",
    "    prompt = \"Classify as 'Safe Email' or 'Phishing Email'.\\n\\nExample:/n\"\n",
    "    prompt += f\"Email: {ex_snippet}\\nAnswer: {example_label}\\n\\n\"\n",
    "    prompt += f\"Now classify:\\nEmail: {test_snippet}\\nAnswer:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=10,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    ans = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    prediction = ans.strip()\n",
    "    return prediction\n",
    "\n",
    "# ============================================================\n",
    "# FEW-SHOT: Multiple Examples\n",
    "# ============================================================\n",
    "\n",
    "def classify_few_shot(text, examples):\n",
    "    \"\"\"\n",
    "    Few-shot classification: provide multiple examples.\n",
    "    \"\"\"\n",
    "    test_snippet = text[:150] if len(text) > 150 else text\n",
    "    \n",
    "    prompt = \"Classify as 'Safe Email' or 'Phishing Email'.\\n\\nExamples:\\n\"\n",
    "    for ex in examples[:2]:  # Use fewer examples for T5\n",
    "        ex_snippet = ex[\"text\"][:150]\n",
    "        prompt += f\"Email: {ex_snippet}\\nAnswer: {ex['label']}\\n\\n\"\n",
    "    prompt += f\"Now classify:\\nEmail: {test_snippet}\\nAnswer:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=10,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    ans = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    prediction = ans.strip()\n",
    "    return prediction\n",
    "\n",
    "# Prepare examples for one-shot and few-shot\n",
    "safe_example = None\n",
    "phishing_example = None\n",
    "few_shot_examples = []\n",
    "safe_count = 0\n",
    "phishing_count = 0\n",
    "\n",
    "for item in ds_splits[\"train\"]:\n",
    "    if item[\"label\"] == \"Safe Email\":\n",
    "        if safe_example is None:\n",
    "            safe_example = {\"text\": item[\"text\"], \"label\": item[\"label\"]}\n",
    "        if safe_count < 2:\n",
    "            few_shot_examples.append({\"text\": item[\"text\"], \"label\": item[\"label\"]})\n",
    "            safe_count += 1\n",
    "    elif item[\"label\"] == \"Phishing Email\":\n",
    "        if phishing_example is None:\n",
    "            phishing_example = {\"text\": item[\"text\"], \"label\": item[\"label\"]}\n",
    "        if phishing_count < 2:\n",
    "            few_shot_examples.append({\"text\": item[\"text\"], \"label\": item[\"label\"]})\n",
    "            phishing_count += 1\n",
    "    if safe_example and phishing_example and safe_count >= 2 and phishing_count >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3039493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all three approaches on the same example\n",
    "test_sample = ds_splits[\"test\"][3]\n",
    "\n",
    "# Run predictions\n",
    "# To normalize the predictions, use the normalize_prediction function\n",
    "zero_shot_pred = classify_zero_shot(test_sample[\"text\"])\n",
    "one_shot_pred = classify_one_shot(test_sample[\"text\"], phishing_example[\"text\"], phishing_example[\"label\"])\n",
    "few_shot_pred = classify_few_shot(test_sample[\"text\"], few_shot_examples)\n",
    "\n",
    "# Display results\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'TEST EMAIL: {test_sample[\"text\"][:400]}...')\n",
    "print(f'ACTUAL LABEL: {test_sample[\"label\"]}')\n",
    "print(dash_line)\n",
    "print(f'ZERO-SHOT:  {zero_shot_pred}')\n",
    "print(f'ONE-SHOT:   {one_shot_pred}')\n",
    "print(f'FEW-SHOT:   {few_shot_pred}')\n",
    "print(dash_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacb2f6f-1159-4e7b-a6b2-ce8c0fdacbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import gc\n",
    "\n",
    "# Evaluate on full test set or subsample to save time\n",
    "test_subsample = ds_splits[\"test\"]#.shuffle(seed=42).select(range(10))\n",
    "\n",
    "print(f'Evaluating zero-shot model on {len(test_subsample)} test examples...')\n",
    "print('This may take a few minutes...\\n')\n",
    "\n",
    "zero_shot_predictions = []\n",
    "one_shot_predictions = []\n",
    "few_shot_predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Process with periodic memory cleanup\n",
    "for i, example in enumerate(tqdm(test_subsample, desc=\"Testing\")):\n",
    "    zero_shot_predictions.append(classify_zero_shot(example[\"text\"]))\n",
    "    one_shot_predictions.append(classify_one_shot(example[\"text\"], phishing_example[\"text\"], phishing_example[\"label\"]))\n",
    "    few_shot_predictions.append(classify_few_shot(example[\"text\"], few_shot_examples))\n",
    "    true_labels.append(example[\"label\"])\n",
    "    \n",
    "    # Clear cache every 50 examples\n",
    "    if (i + 1) % 50 == 0:\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        elif device.type == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# Final cleanup\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "elif device.type == \"mps\":\n",
    "    torch.mps.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Compute metrics\n",
    "zero_shot_accuracy = accuracy_score(true_labels, zero_shot_predictions)\n",
    "one_shot_accuracy = accuracy_score(true_labels, one_shot_predictions)\n",
    "few_shot_accuracy = accuracy_score(true_labels, few_shot_predictions)\n",
    "\n",
    "# To avoid UndefinedMetricWarning for precision/recall on labels with no predicted/true samples, set zero_division=0\n",
    "classification_report_kwargs = {\"zero_division\": 0}\n",
    "\n",
    "# Display results for zero-shot model\n",
    "print('\\n' + '='*60)\n",
    "print('ZERO-SHOT MODEL EVALUATION RESULTS')\n",
    "print('='*60)\n",
    "print(f'Test set size: {len(test_subsample)} examples')\n",
    "print(f'\\nOverall Accuracy: {zero_shot_accuracy:.3f} ({zero_shot_accuracy*100:.1f}%)')\n",
    "\n",
    "print(f'\\nClassification Report:')\n",
    "print(classification_report(true_labels, zero_shot_predictions, **classification_report_kwargs))\n",
    "print('='*60)\n",
    "\n",
    "# Display results for one-shot model\n",
    "print('\\n' + '='*60)\n",
    "print('ONE-SHOT MODEL EVALUATION RESULTS')\n",
    "print('='*60)\n",
    "print(f'Test set size: {len(test_subsample)} examples')\n",
    "print(f'\\nOverall Accuracy: {one_shot_accuracy:.3f} ({one_shot_accuracy*100:.1f}%)')\n",
    "\n",
    "print(f'\\nClassification Report:')\n",
    "print(classification_report(true_labels, one_shot_predictions, **classification_report_kwargs))\n",
    "print('='*60)\n",
    "\n",
    "# Display results for few-shot model\n",
    "print('\\n' + '='*60)\n",
    "print('FEW-SHOT MODEL EVALUATION RESULTS')\n",
    "print('='*60)\n",
    "print(f'Test set size: {len(test_subsample)} examples')\n",
    "print(f'\\nOverall Accuracy: {few_shot_accuracy:.3f} ({few_shot_accuracy*100:.1f}%)')\n",
    "\n",
    "print(f'\\nClassification Report:')\n",
    "print(classification_report(true_labels, few_shot_predictions, **classification_report_kwargs))\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624249c",
   "metadata": {},
   "source": [
    "## 3. Full Fine-Tuning\n",
    "\n",
    "Full fine-tuning is the process of updating **all model parameters** by training on task-specific data. Unlike in-context learning (which uses examples in prompts), fine-tuning modifies the model's weights to specialize it for your specific task.\n",
    "\n",
    "<img src=\"images/finetuning-llm-diagram.jpg\">\n",
    "\n",
    "### What is Full Fine-Tuning?\n",
    "\n",
    "- **Parameter Updates**: All layers of the model (embeddings, attention, feedforward networks) are trained\n",
    "- **Task Specialization**: The model learns patterns specific to your task (phishing detection)\n",
    "- **Persistent Learning**: Knowledge is stored in the model weights, not in the prompt\n",
    "- **Resource Intensive**: Requires more compute and memory than in-context learning or parameter-efficient methods (like LoRA)\n",
    "\n",
    "### Why Use Full Fine-Tuning?\n",
    "\n",
    "**Advantages:**\n",
    "- **Maximum Performance**: Achieves the highest accuracy by leveraging all model parameters\n",
    "- **Efficient Inference**: No need for long prompts with examples (saves tokens and latency)\n",
    "- **Task-Specific Expertise**: Model becomes specialized in your domain (e.g. email security)\n",
    "- **Better Generalization**: Learns underlying patterns rather than memorizing prompt patterns\n",
    "- **Smaller Context Window**: Doesn't consume context with few-shot examples\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Computational Cost**: Requires GPU/TPU and significant training time (often tens or hundreds of hours)\n",
    "- **Memory Requirements**: Must fit entire model + gradients + optimizer states in memory\n",
    "  - **Storage**: For models up to several billion parameters (e.g., T5-small to T5-3B, or most FLAN-T5 and Llama-2 variants up to ~7B parameters), a full model checkpoint is typically ~100MB-6GB. For larger models (e.g., 13B, 70B+), storage requirements increase proportionally, often exceeding 10GB-40GB per checkpoint.\n",
    "- **Requires Labeled Data**: Need sufficient training examples (~10,000+ for best results)\n",
    "\n",
    "### When to Choose Full Fine-Tuning vs In-Context Learning?\n",
    "\n",
    "| **Scenario** | **Use In-Context Learning** | **Use Full Fine-Tuning** |\n",
    "|--------------|----------------------------|--------------------------|\n",
    "| **Data availability** | Few examples (<100) | Many examples (>1,000) |\n",
    "| **Inference frequency** | Occasional queries | High-volume production |\n",
    "| **Deployment** | API-based (OpenAI, etc.) | Self-hosted models |\n",
    "| **Task complexity** | Simple classification | Complex reasoning |\n",
    "| **Budget/Resources** | Limited compute | GPU access available |\n",
    "\n",
    "### Full Fine-Tuning Process\n",
    "\n",
    "In this section, we'll:\n",
    "1. **Format data** (Section 3.1): Create prompt-target pairs for supervised learning\n",
    "2. **Tokenize sequences** (Section 3.2): Convert text to token IDs with proper input/label separation\n",
    "3. (Optional) **Reduce data size** (Section 3.3): When training a larger model or training on CPU, reduce the amount of data. Even with a 1000 samples, the training can take several hours.\n",
    "5. **Train the model** (Section 3.4): Set up batch size, learning rate, and epochs. Update all model parameters via gradient descent\n",
    "6. **Evaluate results** (Section 3.5): Measure accuracy, precision, recall on test set\n",
    "\n",
    "### Full Fine-Tuning vs LoRA (Parameter-Efficient Fine-Tuning)\n",
    "\n",
    "| **Aspect** | **Full Fine-Tuning** | **LoRA** |\n",
    "|-----------|---------------------|---------|\n",
    "| **Parameters Updated** | 100% (all layers) | ~0.1-1% (low-rank adapters) |\n",
    "| **Training Time** | 30-60 min (FLAN-T5-small) | 10-20 min |\n",
    "| **Memory Usage** | High (8-16GB) | Low (2-4GB) |\n",
    "| **Final Accuracy** | Highest (93-95%) | Slightly lower (91-93%) |\n",
    "| **Model Size** | Full size (~300MB) | Base + adapters (~5MB extra) |\n",
    "| **Best For** | Maximum performance | Fast iteration, limited resources |\n",
    "\n",
    "For this notebook, we use **full fine-tuning** to achieve the best possible phishing detection accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb661e58",
   "metadata": {},
   "source": [
    "### 3.1. Format Data for Fine-Tuning\n",
    "We create **prompt-target pairs** where:\n",
    "- **Prompt**: The question + email text\n",
    "- **Target**: The correct label\n",
    "\n",
    "During fine-tuning, the model learns to generate the correct label when given the prompt. This is different from pre-training where the model just learns next-token prediction on general text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb14f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    \"\"\"Convert each example to a prompt-target pair for training.\"\"\"\n",
    "    prompt = f\"Classify this email as 'Safe Email' or 'Phishing Email': {example['text']}\"\n",
    "    target = example[\"label\"]\n",
    "    return {\"prompt\": prompt, \"target\": target}\n",
    "\n",
    "ds_splits_ft = ds_splits.map(format_example)\n",
    "ds_splits_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ea29c8",
   "metadata": {},
   "source": [
    "### 3.2. Tokenize Data\n",
    "\n",
    "Tokenization converts text into numerical token IDs that the model can process. The approach for our Seq2Seq model:\n",
    "\n",
    "1. Tokenize inputs (prompts) and outputs (labels) separately\n",
    "2. Input max length: 512 tokens, Label max length: 32 tokens\n",
    "3. Labels are the target sequences to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00319bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    \"\"\"Tokenize sequences with DataCollator that handles padding.\"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"prompt\"],\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Tokenize labels\n",
    "    labels = tokenizer(\n",
    "        text_target=batch[\"target\"],\n",
    "        truncation=True,\n",
    "        max_length=32\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = ds_splits_ft.map(tokenize, batched=True, remove_columns=ds_splits_ft[\"train\"].column_names)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244faaae",
   "metadata": {},
   "source": [
    "### 3.3. Reduce Dataset Size for CPU Efficiency\n",
    "\n",
    "For CPU training, we use a reduced subset (20% of data) to make training practical (~4-8 hours instead of 100+ hours).\n",
    "\n",
    "**CPU-Optimized**: This notebook uses the reduced dataset by default for faster training on CPU. For GPU training or maximum accuracy, you can use the full dataset by modifying Section 3.4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reduced subsets (20% of data) - CPU-optimized default\n",
    "train_subset = tokenized[\"train\"].select(range(len(tokenized[\"train\"]) // 5))\n",
    "val_subset = tokenized[\"validation\"].select(range(len(tokenized[\"validation\"]) // 5))\n",
    "\n",
    "print(f'Full dataset: {len(tokenized[\"train\"])} train, {len(tokenized[\"validation\"])} val')\n",
    "print(f'Reduced dataset (CPU-optimized): {len(train_subset)} train, {len(val_subset)} val (20%)')\n",
    "print(f'\\n✓ Using reduced dataset for training to optimize for CPU performance')\n",
    "print(f'  Estimated training time on CPU: ~4-8 hours')\n",
    "print(f'  For full dataset training on GPU, replace train_subset/val_subset with tokenized[\"train\"]/tokenized[\"validation\"] in Section 3.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db254e0",
   "metadata": {},
   "source": [
    "### 3.4. Full Fine-Tuning (CPU-Optimized)\n",
    "\n",
    "Now we train the model on our phishing detection dataset. This will update **all parameters** of the model (unlike LoRA which only updates small adapter matrices - we'll look at that approach later).\n",
    "\n",
    "**CPU-Optimized Training Configuration:**\n",
    "- **Dataset**: 20% of training data (~3,000 examples) for practical CPU training\n",
    "- **Epochs**: 3 full passes through the training data\n",
    "- **Batch size**: 1 per device with gradient accumulation (effective batch size = 4)\n",
    "- **Learning rate**: 2e-5 (small to preserve pre-trained knowledge)\n",
    "- **Gradient checkpointing**: Enabled to reduce memory usage\n",
    "- **Evaluation**: Disabled during training to save time\n",
    "\n",
    "**Training Time Estimates:**\n",
    "- FLAN-T5-small on CPU: ~1 hour (20% dataset)\n",
    "\n",
    "**Note**: This notebook uses reduced dataset by default. For full dataset training, replace `train_subset` with `tokenized[\"train\"]` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8734a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear memory. In case you are running fine-tuning again, this will clear the memory from the previous run.\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d12402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device\n",
    "device = torch.device(device_type)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbcd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# CPU-Optimized Training Configuration\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_full_finetuning_dir,\n",
    "    per_device_train_batch_size=1,  # Smaller batch size for CPU (1 for memory efficiency)\n",
    "    per_device_eval_batch_size=1,  # Smaller batch size for evaluation\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 1*4 = 4 (CPU-optimized)\n",
    "    num_train_epochs=3,  # Number of full passes through the training data\n",
    "    learning_rate=2e-5,  # Small learning rate to preserve pre-trained knowledge\n",
    "    logging_steps=100,  # Log every 100 steps (less frequent for CPU)\n",
    "    evaluation_strategy=\"no\",  # Don't evaluate during training (saves significant time on CPU)\n",
    "    save_strategy=\"epoch\",  # Save model checkpoint at end of each epoch\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    use_cpu=(device_type == \"cpu\"),\n",
    "    save_total_limit=1,  # Only keep the last checkpoint to save disk space\n",
    "    save_only_model=True,  # Only save the model, not the tokenizer\n",
    "    fp16=False,  # Disable fp16 for CPU compatibility\n",
    ")\n",
    "\n",
    "# Create data collator for automatic input padding\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Initialize trainer with REDUCED DATASET (CPU-optimized)\n",
    "# For full dataset on GPU, replace train_subset/val_subset with tokenized[\"train\"]/tokenized[\"validation\"]\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset,  # CPU-optimized: using 20% of data\n",
    "    eval_dataset=val_subset,  # CPU-optimized: using 20% of data\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device configuration\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Trainer device: {training_args.device}\")\n",
    "print(f\"Model type: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325e085-604b-45c7-9862-8e2e0e80f520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start training (CPU-optimized with reduced dataset)\n",
    "print(f'Starting CPU-optimized training on {len(train_subset)} examples (20% of full dataset)...')\n",
    "print(f'Device: {device_type.upper()}')\n",
    "print(f'Checkpoints: {output_full_finetuning_dir}/checkpoint-*\\n')\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print('\\n✓ Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373f29c0",
   "metadata": {},
   "source": [
    "#### Save the Fine-Tuned Model\n",
    "\n",
    "Save the trained model and tokenizer to a dedicated directory for later use. The model can then be loaded for inference or shared with other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef6d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = f\"{output_full_finetuning_dir}/final_model_cpu_20_perc\"\n",
    "\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f'✓ Model saved to: {final_model_path}')\n",
    "print(f'\\nTo load: AutoModelForSeq2SeqLM.from_pretrained(\"{final_model_path}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c8053",
   "metadata": {},
   "source": [
    "### 3.5. Evaluate the Fine-Tuned Model\n",
    "\n",
    "Load the fine-tuned model and test it on examples from the test set. We'll compare the model's predictions with the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b40c567-aa31-45c0-abb8-eab74c8e56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = f\"{output_full_finetuning_dir}/final_model_cpu_20_perc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99b728",
   "metadata": {},
   "source": [
    "### Test on one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Load fine-tuned model\n",
    "full_finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(final_model_path)\n",
    "full_finetuned_tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
    "\n",
    "# Move to device\n",
    "full_finetuned_model = full_finetuned_model.to(device)  # Device set in section 6.\n",
    "\n",
    "def classify_email_finetuned(text):\n",
    "    \"\"\"Classify email using fine-tuned model.\"\"\"\n",
    "    prompt = f\"Classify this email as 'Safe Email' or 'Phishing Email': {text}\"\n",
    "    \n",
    "    inputs = full_finetuned_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = full_finetuned_model.generate(**inputs, max_new_tokens=10)\n",
    "    \n",
    "    result = full_finetuned_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result.strip()\n",
    "\n",
    "# Quick test on one example\n",
    "test_email = ds_splits[\"test\"][3]\n",
    "prediction = classify_email_finetuned(test_email[\"text\"])\n",
    "\n",
    "print(f'Quick test on one example:')\n",
    "print(f'Email: {test_email[\"text\"][:400]}...')\n",
    "print(f'Prediction: {prediction}')\n",
    "print(f'Actual: {test_email[\"label\"]}')\n",
    "print(f'Correct: {prediction == test_email[\"label\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a37854",
   "metadata": {},
   "source": [
    "#### Evaluate on Test Set (CPU-Optimized)\n",
    "\n",
    "On one sample, the result looks good. Now let's evaluate the fine-tuned model on a subset of the test set.\n",
    "\n",
    "The evaluation computes:\n",
    "- **Accuracy**: Overall percentage of correct predictions\n",
    "- **Per-class metrics**: Precision, recall, and F1-score for each class (Safe Email, Phishing Email)\n",
    "\n",
    "**CPU-Optimized**: Subsample the testset for faster evaluation (~5-10 minutes on CPU). For full evaluation (1,867 samples), change the range in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2efb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Memory-efficient classification function\n",
    "def classify_with_memory_management(text):\n",
    "    \"\"\"Classify with memory cleanup to prevent OOM errors.\"\"\"\n",
    "    # Truncate text to save memory\n",
    "    prompt = f\"Classify this email as 'Safe Email' or 'Phishing Email': {text[:400]}\"\n",
    "    \n",
    "    inputs = full_finetuned_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = full_finetuned_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            pad_token_id=full_finetuned_tokenizer.eos_token_id if hasattr(full_finetuned_tokenizer, 'eos_token_id') else None\n",
    "        )\n",
    "    \n",
    "    # Decode only generated tokens\n",
    "    result = full_finetuned_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    prediction = result.strip()\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# CPU-Optimized: Evaluate on subset of test set (300 samples)\n",
    "# For full evaluation, use: test_subsample = ds_splits[\"test\"]\n",
    "# test_subsample = ds_splits[\"test\"].shuffle(seed=42).select(range(300))\n",
    "print(f'Evaluating fine-tuned model on {len(test_subsample)} test examples (CPU-optimized)...')\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Process with periodic memory cleanup\n",
    "for i, example in enumerate(tqdm(test_subsample, desc=\"Testing\")):\n",
    "    pred = classify_with_memory_management(example[\"text\"])\n",
    "    predictions.append(pred)\n",
    "    true_labels.append(example[\"label\"])\n",
    "    \n",
    "    # Clear cache every 50 examples\n",
    "    if (i + 1) % 50 == 0:\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        elif device.type == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# Final cleanup\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "elif device.type == \"mps\":\n",
    "    torch.mps.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "# Display results\n",
    "print('\\n' + '='*60)\n",
    "print('FINE-TUNED MODEL EVALUATION RESULTS')\n",
    "print('='*60)\n",
    "print(f'Test set size: {len(test_subsample)} examples')\n",
    "print(f'\\nOverall Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)')\n",
    "\n",
    "print(f'\\nClassification Report:')\n",
    "print(classification_report(true_labels, predictions))\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc169b",
   "metadata": {},
   "source": [
    "### Final notes to full fine-tuning\n",
    "\n",
    "**Catastrophic forgetting**\n",
    "- We updated all model parameters => the model likely lost some of it's previous capabilities. It is now specialized for phishing classification but might perform much worse on e.g., summarization, than the base model before fine-tuning.\n",
    "- In our case (using the model for specific task) it is not an issue. We don't need a general model.\n",
    "- If you need to maintain the base model's capabilities, use PEFT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac89a47",
   "metadata": {},
   "source": [
    "## 4. Parameter-Efficient Fine-Tuning (PEFT) with LoRA\n",
    "<img src=\"images/peft-lora-diagram.jpg\">\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning (PEFT) technique that enables efficient adaptation of large language models by injecting trainable low-rank matrices into existing model weights, while keeping most of the original model parameters frozen. This significantly reduces the number of trainable parameters and accelerates training without sacrificing performance.\n",
    "\n",
    "Full fine-tuning vs LoRA in trainable parameters: \n",
    "- **Full fine-tuning**\n",
    "  - *all* model parameters (often hundreds of millions or even billions) are updated during training,\n",
    "  - requires significant compute and storage\n",
    "  - grants the model maximum flexibility to adapt to the new task\n",
    "  - risks overwriting previously learned information (catastrophic forgetting)\n",
    "- **LoRA**\n",
    "  - freezes the original (pre-trained) model weights\n",
    "  - injects small, trainable \"adapter\" layers (low-rank matrices) into specific parts of the model\n",
    "    - often less than 1% of the total parameters\n",
    "  - saves memory\n",
    "  - accelerates training\n",
    "  - uses less data\n",
    "  - more accessible for users with limited hardware\n",
    "\n",
    " | Approach               | Trainable Parameters | Storage Required | Base Model Retained? |\n",
    " |------------------------|---------------------|------------------|----------------------|\n",
    " | Full Fine-Tuning       | 100%                | High             | No                   |\n",
    " | LoRA / PEFT            | ~0.1%-2%            | Low              | Yes                  |\n",
    " \n",
    "\n",
    "\n",
    "In the following section, we'll explore how to apply LoRA to our phishing classification model using the `peft` library.\n",
    "\n",
    "**References:**\n",
    "- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
    "- [Hugging Face PEFT documentation](https://huggingface.co/docs/peft/index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec54f9",
   "metadata": {},
   "source": [
    "### 4.1. Prepare LoRA Training Data\n",
    "\n",
    "For LoRA training on FLAN-T5, we'll reuse the tokenized dataset from Section 3.2. The same data format works for both full fine-tuning and LoRA since both use the same base model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8dd805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU-Optimized: Use 20% of data for practical LoRA training on CPU\n",
    "# For full training on GPU, use the full tokenized dataset\n",
    "lora_train_size = len(tokenized[\"train\"]) // 5  # 20% of training data\n",
    "lora_val_size = len(tokenized[\"validation\"]) // 5  # 20% of validation data\n",
    "\n",
    "lora_train_ds = tokenized[\"train\"].select(range(lora_train_size))\n",
    "lora_val_ds = tokenized[\"validation\"].select(range(lora_val_size))\n",
    "\n",
    "print(f'LoRA Training dataset: {len(lora_train_ds)} examples (20% - CPU-optimized)')\n",
    "print(f'LoRA Validation dataset: {len(lora_val_ds)} examples (20% - CPU-optimized)')\n",
    "print(f'Note: For full dataset training, modify lora_train_size and lora_val_size above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070325a6",
   "metadata": {},
   "source": [
    "### 4.2. Load Base Model and Apply LoRA Adapters\n",
    "\n",
    "We'll load a fresh copy of the base model and apply LoRA adapters to it. For FLAN-T5, we'll target the attention layers (q, k, v, o projections).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6bdfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Load a fresh base model for LoRA training\n",
    "lora_base_model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Configure LoRA parameters for FLAN-T5\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                          # Rank of LoRA adapters (higher = more capacity = more trainable parameters).\n",
    "    lora_alpha=16,                # Scaling factor for LoRA weights.\n",
    "    target_modules=['q', 'v'],    # Apply LoRA to attention query/value projections. q (query) is the input to the model, v (value) is the output, k (key) is the output of the previous layer, and o (output) is the input to the next layer.\n",
    "    lora_dropout=0.05,            # Dropout for regularization\n",
    "    bias='none',                  # Don't adapt bias terms\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM  # Task type: sequence-to-sequence language modeling\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters to the base model\n",
    "lora_model = get_peft_model(lora_base_model, lora_config)\n",
    "print('\\nTrainable parameters:')\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca28fa57",
   "metadata": {},
   "source": [
    "### 4.3. Configure Training and Train LoRA Adapters (CPU-Optimized)\n",
    "\n",
    "LoRA training is much faster than full fine-tuning because we only update ~1% of parameters.\n",
    "\n",
    "**Training Time Estimate (20% dataset):**\n",
    "- CPU: ~20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device\n",
    "lora_model = lora_model.to(device)\n",
    "\n",
    "# Configure LoRA training arguments (CPU-Optimized)\n",
    "lora_output_dir = f\"{model_name_clean}_lora_cpu_20_perc\"\n",
    "\n",
    "lora_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=lora_output_dir,\n",
    "    per_device_train_batch_size=2,   # CPU-optimized batch size\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,    # Effective batch size = 8 (CPU-optimized)\n",
    "    num_train_epochs=3,               # 3 epochs with reduced dataset\n",
    "    learning_rate=2e-5,               # Standard learning rate for LoRA\n",
    "    logging_steps=100,                # Less frequent logging for CPU\n",
    "    evaluation_strategy=\"no\",               # No evaluation during training to save time\n",
    "    save_strategy=\"epoch\",            # Save at end of each epoch\n",
    "    use_cpu=(device_type == \"cpu\"),\n",
    "    save_total_limit=1,               # Keep only last checkpoint\n",
    "    save_only_model=True,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,                       # Disable fp16 for CPU compatibility\n",
    ")\n",
    "\n",
    "# Create data collator (reuse from full fine-tuning)\n",
    "lora_data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=lora_model)\n",
    "\n",
    "# Initialize trainer\n",
    "lora_trainer = Seq2SeqTrainer(\n",
    "    model=lora_model,\n",
    "    args=lora_training_args,\n",
    "    train_dataset=lora_train_ds,     # Using 20% of data (CPU-optimized)\n",
    "    eval_dataset=lora_val_ds,        # Using 20% of data (CPU-optimized)\n",
    "    data_collator=lora_data_collator\n",
    ")\n",
    "\n",
    "print(f'LoRA Trainer initialized (CPU-Optimized)')\n",
    "print(f'Model device: {next(lora_model.parameters()).device}')\n",
    "print(f'Training samples: {len(lora_train_ds)} (20% of full dataset)')\n",
    "print(f'Output directory: {lora_output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start LoRA training (CPU-Optimized)\n",
    "print(f'Starting LoRA training on {len(lora_train_ds)} examples...')\n",
    "print(f'Device: {device_type.upper()}')\n",
    "\n",
    "lora_trainer.train()\n",
    "\n",
    "print('\\n✓ LoRA training complete!')\n",
    "\n",
    "# Save the LoRA adapter\n",
    "lora_adapter_path = f\"{lora_output_dir}/lora_adapter\"\n",
    "lora_model.save_pretrained(lora_adapter_path)\n",
    "print(f'✓ LoRA adapter saved to: {lora_adapter_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d02df",
   "metadata": {},
   "source": [
    "### 4.4. Evaluate LoRA Model and Compare with Other Approaches (CPU-Optimized)\n",
    "\n",
    "Now let's evaluate the LoRA model and compare its performance against:\n",
    "1. **Zero-shot** (base model with no training)\n",
    "2. **Full fine-tuning** (all parameters updated)\n",
    "3. **LoRA** (only adapter parameters updated)\n",
    "\n",
    "**CPU-Optimized**: Use sample of testset for faster evaluation (~10-15 minutes on CPU). For full evaluation, modify eval_size below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb93ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Helper function for generating predictions\n",
    "def generate_prediction(model_to_use, text, max_new_tokens=10):\n",
    "    \"\"\"Generate prediction from any model.\"\"\"\n",
    "    prompt = f\"Classify this email as 'Safe Email' or 'Phishing Email': {text[:400]}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_to_use.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result.strip()\n",
    "\n",
    "# Prepare models for evaluation\n",
    "\n",
    "# 1. Zero-shot model (base model, no training)\n",
    "zero_shot_model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)\n",
    "zero_shot_model.eval()\n",
    "\n",
    "# 2. LoRA model (just trained)\n",
    "# Load the LoRA model from the output folder and set to eval mode\n",
    "# lora_model = PeftModel.from_pretrained(zero_shot_model, lora_adapter_path).to(device)\n",
    "lora_model.eval()\n",
    "\n",
    "# 3. Full fine-tuned model (we already loaded it in Section 3.5)\n",
    "# full_finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(final_model_path).to(device)\n",
    "full_finetuned_model.eval()\n",
    "print(\"Models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe31181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU-Optimized: Evaluate on subset of test set (300 samples)\n",
    "# For full evaluation on GPU, use: eval_size = len(ds_splits[\"test\"])\n",
    "# eval_size = 300  # CPU-optimized: 300 samples for faster evaluation\n",
    "# test_subsample = ds_splits[\"test\"].shuffle(seed=42).select(range(eval_size))\n",
    "\n",
    "print(f'Evaluating on {len(test_subsample)} test samples (CPU-optimized)...')\n",
    "\n",
    "# Storage for predictions\n",
    "zero_shot_preds = []\n",
    "lora_preds = []\n",
    "full_preds = []\n",
    "true_labels = []\n",
    "\n",
    "# Evaluate each model\n",
    "for i, example in enumerate(tqdm(test_subsample, desc=\"Evaluating models\")):\n",
    "    text = example[\"text\"]\n",
    "    true_label = example[\"label\"]\n",
    "    true_labels.append(true_label)\n",
    "    \n",
    "    # Zero-shot prediction\n",
    "    zs_pred = generate_prediction(zero_shot_model, text)\n",
    "    zero_shot_preds.append(zs_pred)\n",
    "    \n",
    "    # LoRA prediction\n",
    "    lora_pred = generate_prediction(lora_model, text)\n",
    "    lora_preds.append(lora_pred)\n",
    "    \n",
    "    # Full fine-tuned prediction\n",
    "    full_pred = generate_prediction(full_finetuned_model, text)\n",
    "    full_preds.append(full_pred)\n",
    "    \n",
    "    # Periodic memory cleanup\n",
    "    if (i + 1) % 50 == 0:\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        elif device.type == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# Final cleanup\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "elif device.type == \"mps\":\n",
    "    torch.mps.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print('\\n✓ Evaluation complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison results\n",
    "print('\\n' + '='*70)\n",
    "print('COMPARISON: Zero-Shot vs LoRA vs Full Fine-Tuning')\n",
    "print('='*70)\n",
    "\n",
    "# Compute accuracies\n",
    "zero_shot_acc = accuracy_score(true_labels, zero_shot_preds)\n",
    "lora_acc = accuracy_score(true_labels, lora_preds)\n",
    "\n",
    "print(f'\\n Test set size: {len(test_subsample)} examples\\n')\n",
    "\n",
    "# Zero-shot results\n",
    "print('1) ZERO-SHOT (No Training)')\n",
    "print('-' * 70)\n",
    "print(f'   Accuracy: {zero_shot_acc:.3f} ({zero_shot_acc*100:.1f}%)')\n",
    "print(f'\\n   Classification Report:')\n",
    "for line in classification_report(true_labels, zero_shot_preds, zero_division=0).split('\\n'):\n",
    "    if line.strip():\n",
    "        print(f'   {line}')\n",
    "\n",
    "# LoRA results\n",
    "print(f'\\n2) LoRA (Parameter-Efficient Fine-Tuning)')\n",
    "print('-' * 70)\n",
    "print(f'   Accuracy: {lora_acc:.3f} ({lora_acc*100:.1f}%)')\n",
    "print(f'   Improvement over zero-shot: {(lora_acc - zero_shot_acc)*100:+.1f}%')\n",
    "print(f'\\n   Classification Report:')\n",
    "for line in classification_report(true_labels, lora_preds, zero_division=0).split('\\n'):\n",
    "    if line.strip():\n",
    "        print(f'   {line}')\n",
    "\n",
    "# Full fine-tuned results\n",
    "full_acc = accuracy_score(true_labels, full_preds)\n",
    "print(f'\\n3) FULL FINE-TUNING (All Parameters Updated)')\n",
    "print('-' * 70)\n",
    "print(f'   Accuracy: {full_acc:.3f} ({full_acc*100:.1f}%)')\n",
    "print(f'   Improvement over zero-shot: {(full_acc - zero_shot_acc)*100:+.1f}%')\n",
    "print(f'   Improvement over LoRA: {(full_acc - lora_acc)*100:+.1f}%')\n",
    "print(f'\\n   Classification Report:')\n",
    "for line in classification_report(true_labels, full_preds, zero_division=0).split('\\n'):\n",
    "    if line.strip():\n",
    "        print(f'   {line}')\n",
    "\n",
    "# Summary comparison\n",
    "print(f'\\n' + '='*70)\n",
    "print('SUMMARY')\n",
    "print('='*70)\n",
    "print(f'Zero-shot       → LoRA:          {(lora_acc - zero_shot_acc)*100:+.1f}% improvement')\n",
    "print(f'LoRA            → Full:          {(full_acc - lora_acc)*100:+.1f}% improvement')\n",
    "print(f'Zero-shot       → Full:          {(full_acc - zero_shot_acc)*100:+.1f}% total improvement')\n",
    "\n",
    "print(f'\\n Training Efficiency:')\n",
    "print(f'   LoRA:  ~1% parameters trained, {lora_acc*100:.1f}% accuracy')\n",
    "print(f'   Full: 100% parameters trained, {full_acc*100:.1f}% accuracy')\n",
    "print(f'   Trade-off: LoRA achieves {(lora_acc/full_acc)*100:.1f}% of full accuracy with 1% of parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c3e3d",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "**When to use LoRA:**\n",
    "- Limited computational resources (CPU or smaller GPU)\n",
    "- Need to maintain multiple task-specific models\n",
    "- Want faster iteration and experimentation\n",
    "- Need to preserve base model capabilities (avoid catastrophic forgetting)\n",
    "- Working with limited training data\n",
    "\n",
    "**When to use Full Fine-Tuning:**\n",
    "- Maximum performance is critical\n",
    "- Have sufficient computational resources (GPU with 8GB+ memory)\n",
    "- Task is significantly different from pre-training\n",
    "- Have large amounts of training data (10,000+ examples)\n",
    "- Don't need to maintain base model capabilities\n",
    "\n",
    "**Typical Results (for this task with 20% dataset):**\n",
    "- Zero-shot: ~30-40% accuracy (no training required)\n",
    "- LoRA: ~85-90% accuracy (1% parameters, ~20 minutes on CPU)\n",
    "- Full: ~85-90% accuracy (100% parameters, ~1 hour on CPU)\n",
    "\n",
    "**CPU-Optimized Training Times:**\n",
    "- Full Fine-Tuning: ~1 hour (20% dataset)\n",
    "- LoRA: ~20 minutes (20% dataset)\n",
    "- Evaluation: ~5 minutes per approach (full testset)\n",
    "\n",
    "**LoRA achieves ~68% of full fine-tuning performance with only ~1% of trainable parameters and ~50% training time!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e55f0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
